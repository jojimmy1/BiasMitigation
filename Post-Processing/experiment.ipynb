{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/li3975/cs587\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# import calib_eq_odds as eq\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "sys.path.append(cwd + \"/Post-Processing\")\n",
    "from calib_eq_odds import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "fn_rate = 1\n",
    "fp_rate = 0\n",
    "\n",
    "kiva_label = \"dataAug/kiva_label.csv\"\n",
    "student_label = \"dataAug/student_label.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(kiva_label)\n",
    "data['repayment_interval'] = data['repayment_interval'].replace(2, 1)\n",
    "train_data = data [0:20000]\n",
    "target = train_data['repayment_interval']\n",
    "target = torch.tensor(target.values).float()\n",
    "X = train_data.drop(['repayment_interval'], axis=1)\n",
    "X = torch.tensor(X.values).float()\n",
    "gender_index = data.columns.get_loc('borrower_genders')\n",
    "val_data = data[20000: 40000]\n",
    "val_target = val_data['repayment_interval']\n",
    "val_target = torch.tensor(val_target.values).float()\n",
    "val_X = val_data.drop(['repayment_interval'], axis=1)\n",
    "val_X = torch.tensor(val_X.values).float()\n",
    "test_data = data[40000:]\n",
    "test_target = test_data['repayment_interval']\n",
    "test_target = torch.tensor(test_target.values).float()\n",
    "test_X = test_data.drop(['repayment_interval'], axis=1)\n",
    "test_X = torch.tensor(test_X.values).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the parent constructor to initialize the nn.Module\n",
    "        super(LogisticRegression, self).__init__()\n",
    "\n",
    "        # Define a linear transformation layer (weights and bias) for the model.\n",
    "        # `x_female.shape[1]` specifies the number of input features, and `out_features=1` indicates a single output for binary classification.\n",
    "        # The bias term is included by setting bias=True, allowing the model to learn an intercept.\n",
    "        self.w = nn.Linear(9, out_features=1, bias=True)\n",
    "\n",
    "        # Initialize a sigmoid activation function, which will be used to convert logits to probabilities.\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass for the model.\n",
    "        # Apply the linear transformation to the input `x`.\n",
    "        w = self.w(x)\n",
    "\n",
    "        # Apply the sigmoid function to the output of the linear layer.\n",
    "        # This converts the logits (raw output of the linear layer) to probabilities in the range (0, 1).\n",
    "        output = self.sigmoid(w)\n",
    "\n",
    "        # Return the output probabilities.\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = LogisticRegression()\n",
    "    ceriation = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = ceriation(output.squeeze(1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, 100, loss.item()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:90.7762\n",
      "epoch [2/100], loss:88.7517\n",
      "epoch [3/100], loss:87.3981\n",
      "epoch [4/100], loss:86.0003\n",
      "epoch [5/100], loss:84.6232\n",
      "epoch [6/100], loss:82.8495\n",
      "epoch [7/100], loss:80.6607\n",
      "epoch [8/100], loss:77.6440\n",
      "epoch [9/100], loss:74.4764\n",
      "epoch [10/100], loss:71.1278\n",
      "epoch [11/100], loss:67.0021\n",
      "epoch [12/100], loss:62.3932\n",
      "epoch [13/100], loss:56.8868\n",
      "epoch [14/100], loss:50.8296\n",
      "epoch [15/100], loss:43.3504\n",
      "epoch [16/100], loss:34.1392\n",
      "epoch [17/100], loss:24.0827\n",
      "epoch [18/100], loss:13.4962\n",
      "epoch [19/100], loss:4.7600\n",
      "epoch [20/100], loss:4.5659\n",
      "epoch [21/100], loss:5.3234\n",
      "epoch [22/100], loss:5.5040\n",
      "epoch [23/100], loss:5.5213\n",
      "epoch [24/100], loss:5.5257\n",
      "epoch [25/100], loss:5.5258\n",
      "epoch [26/100], loss:5.5300\n",
      "epoch [27/100], loss:5.5300\n",
      "epoch [28/100], loss:5.5300\n",
      "epoch [29/100], loss:5.5300\n",
      "epoch [30/100], loss:5.5300\n",
      "epoch [31/100], loss:5.5300\n",
      "epoch [32/100], loss:5.5300\n",
      "epoch [33/100], loss:5.5300\n",
      "epoch [34/100], loss:5.5300\n",
      "epoch [35/100], loss:5.5300\n",
      "epoch [36/100], loss:5.5300\n",
      "epoch [37/100], loss:5.5300\n",
      "epoch [38/100], loss:5.5300\n",
      "epoch [39/100], loss:5.5300\n",
      "epoch [40/100], loss:5.5300\n",
      "epoch [41/100], loss:5.5300\n",
      "epoch [42/100], loss:5.5300\n",
      "epoch [43/100], loss:5.5300\n",
      "epoch [44/100], loss:5.5300\n",
      "epoch [45/100], loss:5.5300\n",
      "epoch [46/100], loss:5.5300\n",
      "epoch [47/100], loss:5.5300\n",
      "epoch [48/100], loss:5.5300\n",
      "epoch [49/100], loss:5.5300\n",
      "epoch [50/100], loss:5.5300\n",
      "epoch [51/100], loss:5.5300\n",
      "epoch [52/100], loss:5.5300\n",
      "epoch [53/100], loss:5.5300\n",
      "epoch [54/100], loss:5.5300\n",
      "epoch [55/100], loss:5.5300\n",
      "epoch [56/100], loss:5.5300\n",
      "epoch [57/100], loss:5.5300\n",
      "epoch [58/100], loss:5.5300\n",
      "epoch [59/100], loss:5.5300\n",
      "epoch [60/100], loss:5.5300\n",
      "epoch [61/100], loss:5.5300\n",
      "epoch [62/100], loss:5.5300\n",
      "epoch [63/100], loss:5.5300\n",
      "epoch [64/100], loss:5.5300\n",
      "epoch [65/100], loss:5.5300\n",
      "epoch [66/100], loss:5.5300\n",
      "epoch [67/100], loss:5.5300\n",
      "epoch [68/100], loss:5.5300\n",
      "epoch [69/100], loss:5.5300\n",
      "epoch [70/100], loss:5.5300\n",
      "epoch [71/100], loss:5.5300\n",
      "epoch [72/100], loss:5.5300\n",
      "epoch [73/100], loss:5.5300\n",
      "epoch [74/100], loss:5.5300\n",
      "epoch [75/100], loss:5.5300\n",
      "epoch [76/100], loss:5.5300\n",
      "epoch [77/100], loss:5.5300\n",
      "epoch [78/100], loss:5.5300\n",
      "epoch [79/100], loss:5.5300\n",
      "epoch [80/100], loss:5.5300\n",
      "epoch [81/100], loss:5.5300\n",
      "epoch [82/100], loss:5.5300\n",
      "epoch [83/100], loss:5.5300\n",
      "epoch [84/100], loss:5.5300\n",
      "epoch [85/100], loss:5.5300\n",
      "epoch [86/100], loss:5.5300\n",
      "epoch [87/100], loss:5.5300\n",
      "epoch [88/100], loss:5.5300\n",
      "epoch [89/100], loss:5.5300\n",
      "epoch [90/100], loss:5.5300\n",
      "epoch [91/100], loss:5.5300\n",
      "epoch [92/100], loss:5.5300\n",
      "epoch [93/100], loss:5.5300\n",
      "epoch [94/100], loss:5.5300\n",
      "epoch [95/100], loss:5.5300\n",
      "epoch [96/100], loss:5.5300\n",
      "epoch [97/100], loss:5.5300\n",
      "epoch [98/100], loss:5.5300\n",
      "epoch [99/100], loss:5.5300\n",
      "epoch [100/100], loss:5.5300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with no grad\n",
    "group_0_val_pred = []\n",
    "group_1_val_pred = []\n",
    "group_0_val_gt= []\n",
    "group_1_val_gt = []\n",
    "\n",
    "group_0_test_pred = []\n",
    "group_1_test_pred = []\n",
    "group_0_test_gt= []\n",
    "group_1_test_gt = []\n",
    "\n",
    "def evaluated_nn(model, val_X, val_target, test_X, test_target):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in zip(val_X, val_target):\n",
    "            y_pred = model(x)\n",
    "            if x[gender_index] == 0:\n",
    "                group_0_val_pred.append(y_pred.item())\n",
    "                group_0_val_gt.append(y.item())\n",
    "            else:\n",
    "                group_1_val_pred.append(y_pred.item())\n",
    "                group_1_val_gt.append(y.item())\n",
    "            # print(f'Prediction: {y_pred.item()}, True: {y.item()}')\n",
    "        \n",
    "        for x, y in zip(test_X, test_target):\n",
    "            y_pred = model(x)\n",
    "            if x[gender_index] == 0:\n",
    "                group_0_test_pred.append(y_pred.item())\n",
    "                group_0_test_gt.append(y.item())\n",
    "            else:\n",
    "                group_1_test_pred.append(y_pred.item())\n",
    "                group_1_test_gt.append(y.item())\n",
    "            # print(f'Prediction: {y_pred.item()}, True: {y.item()}')\n",
    "\n",
    "evaluated_nn(model, val_X, val_target, test_X, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_0_val_model = Model(np.array(group_0_val_pred), np.array(group_0_val_gt))\n",
    "group_1_val_model = Model(np.array(group_1_val_pred), np.array(group_1_val_gt))\n",
    "group_0_test_model = Model(np.array(group_0_test_pred), np.array(group_0_test_gt))\n",
    "group_1_test_model = Model(np.array(group_1_test_pred), np.array(group_1_test_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original group 0 model:\n",
      "Accuracy:\t0.949\n",
      "F.P. cost:\t1.000\n",
      "F.N. cost:\t0.000\n",
      "Base rate:\t0.949\n",
      "Avg. score:\t1.000\n",
      "\n",
      "Original group 1 model:\n",
      "Accuracy:\t0.780\n",
      "F.P. cost:\t1.000\n",
      "F.N. cost:\t0.000\n",
      "Base rate:\t0.780\n",
      "Avg. score:\t1.000\n",
      "\n",
      "Equalized odds group 0 model:\n",
      "Accuracy:\t0.949\n",
      "F.P. cost:\t1.000\n",
      "F.N. cost:\t0.000\n",
      "Base rate:\t0.949\n",
      "Avg. score:\t1.000\n",
      "\n",
      "Equalized odds group 1 model:\n",
      "Accuracy:\t0.780\n",
      "F.P. cost:\t1.000\n",
      "F.N. cost:\t0.000\n",
      "Base rate:\t0.780\n",
      "Avg. score:\t1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, mix_rates = Model.calib_eq_odds(group_0_val_model, group_1_val_model, fp_rate, fn_rate)\n",
    "calib_eq_odds_group_0_test_model, calib_eq_odds_group_1_test_model = Model.calib_eq_odds(\n",
    "    group_0_test_model, group_1_test_model, fp_rate, fn_rate, mix_rates)\n",
    "print('Original group 0 model:\\n%s\\n' % repr(group_0_test_model))\n",
    "print('Original group 1 model:\\n%s\\n' % repr(group_1_test_model))\n",
    "print('Equalized odds group 0 model:\\n%s\\n' % repr(calib_eq_odds_group_0_test_model))\n",
    "print('Equalized odds group 1 model:\\n%s\\n' % repr(calib_eq_odds_group_1_test_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(student_label)\n",
    "data['repayment_interval'] = data['repayment_interval'].replace(2, 1)\n",
    "train_data = data [0:20000]\n",
    "target = train_data['repayment_interval']\n",
    "target = torch.tensor(target.values).float()\n",
    "X = train_data.drop(['repayment_interval'], axis=1)\n",
    "X = torch.tensor(X.values).float()\n",
    "gender_index = data.columns.get_loc('Gender')\n",
    "val_data = data[20000: 40000]\n",
    "val_target = val_data['repayment_interval']\n",
    "val_target = torch.tensor(val_target.values).float()\n",
    "val_X = val_data.drop(['repayment_interval'], axis=1)\n",
    "val_X = torch.tensor(val_X.values).float()\n",
    "test_data = data[40000:]\n",
    "test_target = test_data['repayment_interval']\n",
    "test_target = torch.tensor(test_target.values).float()\n",
    "test_X = test_data.drop(['repayment_interval'], axis=1)\n",
    "test_X = torch.tensor(test_X.values).float()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
